#+TITLE: Kubernetes
#+AUTHOR: chodkows

* Architecture
** Master Node - Manage, Plan, Schedule, Monitor
*** ETCD
**** ETCD basics
Q: What is ETCD?
A: ETCD is a distributed reliable key-value store that is simple, secure and fast
Q: What is a Key-Value Store?
A:
| Key      | Value    |
|----------+----------|
| Name     | John Doe |
| Age      | 45       |
| Location | New York |
| Salary   | 5000     |

| Name        | Age | Location  |
|-------------+-----+-----------|
| John Doe    |  45 | New York  |
| Dave Smith  |  33 | New York  |
| Aryan Kumar |  10 | New York  |
| Lauren Rob  |  13 | Banaglore |
| Lily Oliver |  15 | Bangalore |

Put Name "John Doe"
Get Name
"John Doe"

Q: How to get started quickly
Install ETCD:
Download, extract, and run
./etcd
ETCD listen by default on 2379
Default client is etcdctl
Q: How to operate ETCD?
#+begin_src bash
./etcdctl set key1 value1
./etcdctl get key1
#+end_src

**** ETCD in kubernetes
etcd store information about:
Nodes, Pods, Configs, Secrets, Accounts, Roles, Bindings, Others

***** Setup in k8s - manual
#+begin_src bash
wget -q --https-only |
    "https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz"
#+end_src
#+begin_src bash
ExecStart=/usr/local/bin/etcd \\
--name ${ETCD_NAME} \\
--cert-file=/etc/etcd/kubernetes.pem \\
--key-file=/etc/etcd/kubernetes-key.pem \\
--peer-cert-file=/etc/etcd/kubernetes.pem \\
--peer-key-file/etc/etcd/kubernetes-key.pem \\
--trusted-ca-file=/etc/etcd/ca.pem \\
--peer-trusted-ca-file=/etc/etcd/ca.pem \\
--per-client-cert-auth \\
--client-cert-auth \\
--initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
--listen-peer-urls https://${INTERNAL_IP}:2380 \\
--listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
--advertise-client-urls https://${INTERNAL_IP}:2379 \\
--initial-cluster-token etcd-cluster-0 \\
--initial-cluster controller-0=https://${CONTROLLER0_IP}:2380,controller-1=https://${CONTROLLER1_IP}:2380 \\
--initial-cluster-state new \\
--data-dir=/var/lib/etcd
#+end_src
***** Setup in k8s- kubeadm
Automatic instalation

#+begin_src bash
kubectl get pods -n kube-system

#+end_src

#+RESULTS:
| NAME                                       | READY | STATUS  | RESTARTS | AGE   |      |      |
| coredns-6d4b75cb6d-lptwm                   | 1/1   | Running |       26 | (177m | ago) | 132d |
| coredns-6d4b75cb6d-s5nzh                   | 1/1   | Running |       26 | (177m | ago) | 132d |
| etcd-kind-control-plane                    | 1/1   | Running |       27 | (177m | ago) | 132d |
| kindnet-mf92h                              | 1/1   | Running |       26 | (177m | ago) | 132d |
| kube-apiserver-kind-control-plane          | 1/1   | Running |       27 | (177m | ago) | 132d |
| kube-controller-manager-kind-control-plane | 1/1   | Running |       27 | (177m | ago) | 132d |
| kube-proxy-4m5fj                           | 1/1   | Running |       26 | (177m | ago) | 132d |
| kube-scheduler-kind-control-plane          | 1/1   | Running |       27 | (177m | ago) | 132d |

#+begin_src bash
kubectl exec etcd-kind-control-plane -n kube-system -- etcdctl get / --prefix --keys-only
#+end_src
#+begin_src basH
    kubectl exec etcd-kind-control-plane -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt  --key /etc/kubernetes/pki/etcd/server.key"

#+end_src


***** ETCD in HA Environment
3 instancje
--initial-cluster controller-0=https://${CONTROLLER0_IP}:2380,controller-1=https://${CONTROLLER1_IP}:2380 \\
***** ETCD versions(Optional) Additional information about ETCDCTL Utility

ETCDCTL is the CLI tool used to interact with ETCD.

ETCDCTL can interact with ETCD Server using 2 API versions - Version 2 and Version 3.  By default its set to use Version 2. Each version has different sets of commands.

For example ETCDCTL version 2 supports the following commands:

    etcdctl backup
    etcdctl cluster-health
    etcdctl mk
    etcdctl mkdir
    etcdctl set


Whereas the commands are different in version 3

    etcdctl snapshot save
    etcdctl endpoint health
    etcdctl get
    etcdctl put


To set the right version of API set the environment variable ETCDCTL_API command

export ETCDCTL_API=3


When API version is not set, it is assumed to be set to version 2. And version 3 commands listed above don't work. When API version is set to version 3, version 2 commands listed above don't work.


Apart from that, you must also specify path to certificate files so that ETCDCTL can authenticate to the ETCD API Server. The certificate files are available in the etcd-master at the following path. We discuss more about certificates in the security section of this course. So don't worry if this looks complex:

    --cacert /etc/kubernetes/pki/etcd/ca.crt
    --cert /etc/kubernetes/pki/etcd/server.crt
    --key /etc/kubernetes/pki/etcd/server.key


So for the commands I showed in the previous video to work you must specify the ETCDCTL API version and path to certificate files. Below is the final form:


    kubectl exec etcd-master -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt  --key /etc/kubernetes/pki/etcd/server.key"

*** kube-scheduler
Decide which pod go on which node
1. Filter Nodes based on cpu and memory requirements
2. Rank Nodes (more resources, better node)

**** View kube-scheduler options- kubeadm
cat /etc/kubernetes/manifests/kube-scheduler.yaml
**** View kube-scheduler options - service
ps -aux | grep kube-scheduler

*** Kube-Controller-Manager
Watch status and remediate situation
**** Node-Controller
Node-Controller via kube-apiserver looks into worker nodes for statuses
Node Monitor Period = 5s
Node Monitor Grace Period = 40s (after this time, node will be signed as unreachable)
POD Eviction Timeout = 5m ()
**** Replication-Controller
is about replica sets
**** View kube-controller-manager options - kubeadm
cat /etc/kubernetes/manifests/kube-controller-manager.yaml
**** View kube-controller-manager options - service
cat /etc/systemd/system/kube-controller-manager.service
ps -aux | grep kube-controller-manager
*** kube-apiserver
**** Creating a pod
1. Autheticate User
2. Validate Request
3. Retrieve data
4. Update ETCD
5. Scheduler
6. Kublet

kubectl vs curl -X POST /api/v1/namespaces/default/pods...[other]
**** Building kube-api server in the hard way
kube-apiserver.service

**** View api-server options - kubeadm
#+begin_src bash
kubectl get pods -n kube-system
#+end_src

#+RESULTS:
| NAME                                       | READY | STATUS  | RESTARTS | AGE    |      |      |
| coredns-6d4b75cb6d-lptwm                   | 1/1   | Running |       26 | (3h25m | ago) | 132d |
| coredns-6d4b75cb6d-s5nzh                   | 1/1   | Running |       26 | (3h25m | ago) | 132d |
| etcd-kind-control-plane                    | 1/1   | Running |       27 | (3h25m | ago) | 132d |
| kindnet-mf92h                              | 1/1   | Running |       26 | (3h25m | ago) | 132d |
| kube-apiserver-kind-control-plane          | 1/1   | Running |       27 | (3h25m | ago) | 132d |
| kube-controller-manager-kind-control-plane | 1/1   | Running |       27 | (3h25m | ago) | 132d |
| kube-proxy-4m5fj                           | 1/1   | Running |       26 | (3h25m | ago) | 132d |
| kube-scheduler-kind-control-plane          | 1/1   | Running |       27 | (3h25m | ago) | 132d |

#+begin_src bash
kubectl exec kube-apiserver-kind-control-plane -n kube-system -- cat /etc/kubernetes/manifests/kube-apiserver.yaml
#+end_src

#+RESULTS:
**** View api-server options
cat /etc/systemd/system/kube-apiserver.service
ps -aux | grep kube-apiserver

** Worker Node - Host Application as Containers
*** Container-runtime-engine(rkt, containerd, Docker)
*** Kubelet
1. Register Node
2. Create Pods
3. Monitor node and pods
**** View kublet optios
ps -aux | grep kubelet

*** Kube-proxy
Run on each node. When service is created, kube-proxy will be updated.

Install kube-proxy and run it as a service.
kube-proxy.service
ExecStart=/usr/local/bin/kube-proxy \\
--config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

Kube proxy can be deployed as deamonset
* POD
** Imperative way

#+begin_src bash
kubectl run nginx --image nginx
#+end_src

#+RESULTS:
: pod/nginx created

#+begin_src bash
kubectl get pods
#+end_src

#+RESULTS:
| NAME      | READY | STATUS  | RESTARTS | AGE |
| myapp-pod | 1/1   | Running |        0 | 26s |

#+begin_src bash
kubectl delete pod nginx
#+end_src

#+RESULTS:
: nginx

** Declarative way

#+begin_src bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:
  - name: nginx-container
    image: nginx
EOF
#+end_src

#+RESULTS:
: pod/myapp-pod created
#+begin_src bash
kubectl delete pod myapp-pod
#+end_src

#+RESULTS:
: myapp-pod
* Replication Controller
** Declarative
#+begin_src bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx
  replicas: 3
EOF
#+end_src

#+RESULTS:
: replicationcontroller/myapp-rc created

#+begin_src bash
kubectl get replicationcontroller
#+end_src

#+RESULTS:

#+begin_src bash
kubectl get pods
#+end_src

#+RESULTS:

#+begin_src bash
kubectl delete replicationcontroller myapp-rc
#+end_src

#+RESULTS:
: myapp-rc
* Replicaset
#+begin_src bash
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx
  replicas: 3
  selector:
    matchLabels:
      type: front-end
EOF
#+end_src

#+RESULTS:
: replicaset.apps/myapp-replicaset created

#+begin_src bash
kubectl get replicaset
#+end_src

#+begin_src bash
kubectl get pods
#+end_src

#+RESULTS:

#+begin_src bash
kubectl delete replicaset myapp-replicaset
#+end_src

#+RESULTS:
: myapp-replicaset

** Labels and Selectors
Replicaset monitor pods and it need to know which node have to monitor.

#+begin_src yaml replicaset-definition.yml
selector:
  matchLabels:
    tier: front-end
#+end_src
#+begin_src yaml pod-definition.yml
metadata:
  name: myapp-pod
  labels:
    tier: front-end
#+end_src
** Scale
#+begin_src yaml in replicaset-definition.yml
replicas: 6
#+end_src
#+begin_src bash
kubeclt replace -f replicaset-definition.yml
#+end_src
#+begin_src bash
kubectl scale --replicas=6 -f replicaset-definiton.yml
#+end_src
#+begin_src bash
kubectl scale --replicas=6 -f replicaset myapp-replicaset
#+end_src
* Deployment
Abstration above replicaset. Add ability to releasing.
** Deploy

#+begin_src bash
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx
  replicas: 3
  selector:
    matchLabels:
      type: front-end
EOF
#+end_src

#+RESULTS:
: deployment.apps/myapp-deployment created

#+begin_src bash
kubectl get deployment
#+end_src

#+RESULTS:

#+begin_src bash
kubectl get pods
#+end_src

#+RESULTS:

#+begin_src bash
kubectl delete deployment myapp-deployment
#+end_src

#+RESULTS:
: myapp-deployment

#+begin_src bash
kubectl get all
#+end_src

#+RESULTS:
| NAME               | TYPE      | CLUSTER-IP | EXTERNAL-IP | PORT(S) |  AGE |
| service/kubernetes | ClusterIP |  10.96.0.1 | <none>      | 443/TCP | 133d |
* Certification tip
Here's a tip!

As you might have seen already, it is a bit difficult to create and edit YAML files. Especially in the CLI. During the exam, you might find it difficult to copy and paste YAML files from browser to terminal. Using the kubectl run command can help in generating a YAML template. And sometimes, you can even get away with just the kubectl run command without having to create a YAML file at all. For example, if you were asked to create a pod or deployment with specific name and image you can simply run the kubectl run command.

Use the below set of commands and try the previous practice tests again, but this time try to use the below commands instead of YAML files. Try to use these as much as you can going forward in all exercises

Reference (Bookmark this page for exam. It will be very handy):

https://kubernetes.io/docs/reference/kubectl/conventions/

Create an NGINX Pod
#+begin_src bash
kubectl run nginx --image=nginx
#+end_src

#+RESULTS:
: pod/nginx created

Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)
#+begin_src bash :results scalar
kubectl run nginx --image=nginx --dry-run=client -o yaml
#+end_src

#+RESULTS:
#+begin_example
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
#+end_example

Create a deployment
#+begin_src bash
kubectl create deployment --image=nginx nginx
#+end_src

#+RESULTS:
: deployment.apps/nginx created

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)
#+begin_src bash :results scalar
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml
#+end_src

#+RESULTS:
#+begin_example
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}
#+end_example

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) with 4 Replicas (--replicas=4)
#+begin_src bash :results scalar
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml
#+end_src

Save it to a file, make necessary changes to the file (for example, adding more replicas) and then create the deployment.
#+begin_src bash
kubectl create -f nginx-deployment.yaml
#+end_src

OR

In k8s version 1.19+, we can specify the --replicas option to create a deployment with 4 replicas.
#+begin_src bash
kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml
#+end_src
* Services
** NodePort
TargetPort - pod port (if not specified, the same as Port)
Port - service port (mandatory)
NodePort - node port (if not specified, one available from range 30000 - 32767 will be used)

Service have build in load balancer:
Algorithm: Random
SessionAffinity: Yes

#+begin_src bash
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  ports:
  - targetPort: 80
    port: 80
    nodePort: 30008
  selector:
    app: myapp
    type: front-end
EOF
#+end_src

#+RESULTS:
: service/myapp-service created

#+begin_src bash
kubectl get services
#+end_src

#+RESULTS:
| NAME          | TYPE      |   CLUSTER-IP | EXTERNAL-IP | PORT(S)      | AGE  |
| kubernetes    | ClusterIP |    10.96.0.1 | <none>      | 443/TCP      | 133d |
| myapp-service | NodePort  | 10.96.38.111 | <none>      | 80:30008/TCP | 33s  |
#+begin_src bash :results scalar
kubectl get nodes -o wide
#+end_src

#+RESULTS:
: NAME                 STATUS   ROLES           AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION   CONTAINER-RUNTIME
: kind-control-plane   Ready    control-plane   133d   v1.24.0   172.18.0.2    <none>        Ubuntu 21.10   5.17.2-arch3-1   containerd://1.6.4

#+begin_src bash :results scalar
kubectl cluster-info
#+end_src

#+RESULTS:
: Kubernetes control plane is running at https://127.0.0.1:36953
: CoreDNS is running at https://127.0.0.1:36953/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
:
: To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

#+begin_src bash
curl http://172.18.0.2:30008
#+end_src

In case that pods are in sepearete nodes with separete IP addresses, service will also work, and curl for each IP will work.
curl http://172.18.0.2:30008
curl http://172.18.0.3:30008
curl http://172.18.0.4:30008

#+RESULTS:

** ClusterIP
#+begin_src bash
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: backend
spec:
  type: ClusterIP
  ports:
  - targetPort: 80
    port: 80
  selector:
    app: myapp
    type: backend
EOF
#+end_src

#+RESULTS:
: service/backend created

** LoadBalancer
* Namespaces

Default namespaces:
1. Default
2. Kube-system
3. Kube-public

Both services(service-one and db-service) inside default namspace:
#+begin_src
mysql.connec("db-service")
#+end_src
Service-one in default, db-service in dev namespace:
#+begin_src
mysql.connect("db-service.dev.svc.cluster.local")
#+end_src

During creation a service, to DNS will be added address as below:
db-service.dev.svc.cluster.local
1. cluster.local - domain
2. svc - subdomain
3. dev - namespace
4. db-service - service name

Create namespace:
 #+begin_src bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: dev
EOF
 #+end_src

 #+RESULTS:
 : namespace/dev created
Or:
#+begin_src bash
kubectl create namespace dev
#+end_src

Set context:
#+begin_src bash
kubectl config set-context $(kubectl config current-context) --namespace=dev
#+end_src

#+RESULTS:
: kind-kind

#+begin_src bash
kubectl get pods
#+end_src

List all pods in all namespaces
#+begin_src bash
kubectl get pods --all-namespaces
#+end_src

#+RESULTS:
| NAMESPACE          | NAME                                       | READY | STATUS  | RESTARTS | AGE  |      |      |
| default            | nginx                                      | 1/1   | Running |        2 | (30m | ago) |  12h |
| default            | nginx-8f458dc5b-pqw8p                      | 1/1   | Running |        2 | (30m | ago) |  12h |
| kube-system        | coredns-6d4b75cb6d-lptwm                   | 1/1   | Running |       28 | (30m | ago) | 133d |
| kube-system        | coredns-6d4b75cb6d-s5nzh                   | 1/1   | Running |       28 | (30m | ago) | 133d |
| kube-system        | etcd-kind-control-plane                    | 1/1   | Running |       29 | (30m | ago) | 133d |
| kube-system        | kindnet-mf92h                              | 1/1   | Running |       28 | (30m | ago) | 133d |
| kube-system        | kube-apiserver-kind-control-plane          | 1/1   | Running |       29 | (30m | ago) | 133d |
| kube-system        | kube-controller-manager-kind-control-plane | 1/1   | Running |       29 | (30m | ago) | 133d |
| kube-system        | kube-proxy-4m5fj                           | 1/1   | Running |       28 | (30m | ago) | 133d |
| kube-system        | kube-scheduler-kind-control-plane          | 1/1   | Running |       29 | (30m | ago) | 133d |
| local-path-storage | local-path-provisioner-9cd9bd544-9ptwr     | 1/1   | Running |       46 | (29m | ago) | 133d |

Resource quota:
#+begin_src bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memort: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi
EOF
#+end_src
* Imperative vs Declarative
Create a pod with name *nginx* and image nginx
#+begin_src bash
kubectl run --image=nginx nginx
#+end_src
Create a deployment with name nginx and image nginx
#+begin_src bash
kubectl create deployment --image=nginx nginx
#+end_src

#+RESULTS:
: deployment.apps/nginx created

Create a service with CluserIP type with port and targetPort 80
#+begin_src bash
kubectl expose deployment nginx --port 80
#+end_src

#+RESULTS:
: service/nginx exposed

Edit deployment
#+begin_src bash
kubectl edit deployment nginx
#+end_src
Scale deployment
#+begin_src bash
kubectl scale deployment nginx --replicas=5
#+end_src
Change image for deployment (nginx= means container name)
#+begin_src bash
kubectl set image deployment nginx nginx=nginx:1.18
#+end_src

#+RESULTS:

Create kubernetes resource
#+begin_src bash
kubectl create -f nginx.yaml
#+end_src
Replace kubernetes resource
#+begin_src bash
kubectl replace -f nginx.yaml
#+end_src
Delete kubernetes resource
#+begin_src bash
kubectl delete -f nginx.yaml
#+end_src

For declarative approach:
#+begin_src bash
kubectl apply -f nginx.yaml
#+end_src
** Certification tips
While you would be working mostly the declarative way - using definition files, imperative commands can help in getting one time tasks done quickly, as well as generate a definition template easily. This would help save considerable amount of time during your exams.

Before we begin, familiarize with the two options that can come in handy while working with the below commands:

--dry-run: By default as soon as the command is run, the resource will be created. If you simply want to test your command , use the --dry-run=client option. This will not create the resource, instead, tell you whether the resource can be created and if your command is right.

-o yaml: This will output the resource definition in YAML format on screen.


Use the above two in combination to generate a resource definition file quickly, that you can then modify and create resources as required, instead of creating the files from scratch.


POD
Create an NGINX Pod
#+begin_src
kubectl run nginx --image=nginx
#+end_src
Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)
#+begin_src
kubectl run nginx --image=nginx --dry-run=client -o yaml
#+end_src

Deployment
Create a deployment
#+begin_src
kubectl create deployment --image=nginx nginx
#+end_src
Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)
#+begin_src
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml
#+end_src
Generate Deployment with 4 Replicas
#+begin_src
kubectl create deployment nginx --image=nginx --replicas=4
#+end_src
You can also scale a deployment using the kubectl scale command.
#+begin_src
kubectl scale deployment nginx --replicas=4
#+end_src
Another way to do this is to save the YAML definition to a file and modify
#+begin_src
kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml
#+end_src
You can then update the YAML file with the replicas or any other field before creating the deployment.

Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379
#+begin_src
kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
#+end_src
(This will automatically use the pod's labels as selectors)
Or
#+begin_src
kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml
#+end_src
(This will not use the pods labels as selectors, instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service)

Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:
#+begin_src
kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml
#+end_src
(This will automatically use the pod's labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port in manually before creating the service with the pod.)
Or
#+begin_src
kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml
#+end_src
(This will not use the pods labels as selectors)

Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot accept a node port. I would recommend going with the kubectl expose command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.
Reference:

https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands

https://kubernetes.io/docs/reference/kubectl/conventions/
* Scheduling
** Manual scheduling
If there is no scheduler, pod will remain with Ready 0/1 with status pending.
We can add *nodeName* directive to pod definition.
This works only for newly created pod.
#+begin_src bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 8080
  nodeName: node02
EOF
#+end_src
If pod is actualy running, we need to create binding object.
#+begin_src bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Binding
metadata:
  name: nginx
target:
  apiVersion: v1
  kind: Node
  name: node02
EOF
#+end_src
#+begin_src bash
curl --header "Content-Type: application/json" --request POST --data '{"apiVersion":"v1", "kind": "Binding" ... }' http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding

#+end_src
** Labels and Selectors

#+begin_src bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
  labels:
    app: App1
    function: Front-end
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 8080
EOF
#+end_src

#+RESULTS:
: pod/simple-webapp created

#+begin_src bash
kubectl get pods --selector app=App1
#+end_src

#+RESULTS:
| NAME          | READY | STATUS  | RESTARTS | AGE |
| simple-webapp | 1/1   | Running |        0 | 11s |
** Taints and Tolerations
Taint is set on node side.
Tolerations is set on pod side.

kubectl taint nodes node-name key=value:taint-effect
#+begin_src bash
kubectl taint nodes node1 app=blue:NoSchedule
#+end_src

Untaint node node01 (by adding - sign at the end)
#+begin_src bash
kubectl taint nodes node1 app=blue:NoSchedule-
#+end_src

Taint effects:
1. NoSchedule - pod not be schedule on the node
2. PreferNoSchedule - try avoid placing node on the node
3. NoExecute - new pod will not be sheduled on the node and existing pod will be evicted if not tolerate the taint

Tolerations:
Each values in tolerations need to be in double quotes (""):
#+begin_src bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 8080
  tolerations:
  - key: "app"
    operator: "Equal"
    value: "blue"
    effect: "NoSchedule"
EOF
#+end_src

Master node have own taints. Kind cluster have not taints on kind-control-plane node
#+begin_src bash
kubectl get nodes
#+end_src

#+RESULTS:
| NAME               | STATUS | ROLES         |  AGE | VERSION |
| kind-control-plane | Ready  | control-plane | 133d | v1.24.0 |

#+begin_src bash
kubectl describe node kind-control-plane | grep Taint
#+end_src

#+RESULTS:
: Taints:             <none>
** Nodes selector
#+begin_src bash
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 8080
  nodeSelector:
    size: Large
EOF
#+end_src

#+begin_src bash
kubectl label nodes node-1 size=Large
#+end_src
** Node affinity
Advanced capabilities for selecting nodes for pod

Below snippet is equivalent of the above one
#+begin_src bash
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 8080
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values:
            - Large
EOF
#+end_src

Apply pod if node is medium or large
#+begin_src bash
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values:
            - Medium
            - Large
#+end_src

Apply pod only if node is not small
#+begin_src bash
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: NotIn
            values:
            - Small
#+end_src

Apply pod to node if such (size) label exists
#+begin_src bash
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: Exists
#+end_src

Node affinity types:
Available:
1. requiredDuringSchedulingIgnoredDuringExecution
2. prefferedDuringSchedulingIgnoredDuringExecution
 Planned:
3. requiredDuringSchedulingRequredDuringExecution

| DuringScheduling | DuringExectuion | no labels in node        | remove label during run |
|------------------+-----------------+--------------------------+-------------------------|
| Required         | Ignored         | not scheduled            | will run                |
| Preffered        | Ignored         | scheduled on random node | will run                |
| Required         | Required        | not scheduled            | pod will be removed     |
** Resource Requirements and Limits
1m CPU - minimal amout of cpu
1vCPU - default k8s cpu limit
512Mi - default k8s memory limit

Resource Request: Minimal amout of resources which are needed for pod
Scheduler will take this information and will check that there is a place to put this pod into node based on resource requests.

In the previous lecture, I said - "When a pod is created the containers are assigned a default CPU request of .5 and memory of 256Mi". For the POD to pick up those defaults you must have first set those as default values for request and limit by creating a LimitRange in that namespace.

#+begin_src yaml
apiVersion: v1
kind: LimitRange
metadata:
    name: mem-limit-range
spec:
    limits:
    - default:
        memory: 512Mi
    defaultRequest:
        memory: 256Mi
    type: Container
#+end_src

https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/

#+begin_src yaml
apiVersion: v1
kind: LimitRange
metadata:
    name: cpu-limit-range
spec:
    limits:
    - default:
        cpu: 1
    defaultRequest:
        cpu: 0.5
    type: Container
#+end_src

https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/


References:

https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource
